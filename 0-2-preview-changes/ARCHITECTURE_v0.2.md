# Architecture v0.2: Dynamic Research Tree & Multi-Resolution Retrieval

## Executive Summary

Version 0.2 represents a paradigm shift from a linear, brute-force pipeline to a **Dynamic Cognitive Agent**. Instead of blindly consuming full articles, the system now emulates human research behaviors: scanning tables of contents, selectively reading relevant sections, and structuring learned information into a hierarchical mental model.

This architecture addresses the "Lost-in-the-Middle" phenomenon and Context Window overload by implementing **Multi-Resolution Retrieval** and **Dynamic Knowledge Classification** (inspired by *ChatIndex* logic).

---

## 1. Core Philosophy

The system operates on three foundational pillars designed to maximize reasoning accuracy while minimizing token consumption:

### A. The "Research Tree" (Dynamic Memory)
Unlike traditional flat key-value stores, the agent's memory is a **Hierarchical Tree**.
*   **Concept:** Information is not static; it has relationships.
*   **Mechanism:** When the agent learns a fact (e.g., "Christopher Nolan directed Inception"), it does not just dump it into a list. It dynamically creates a node (e.g., Topic: "Director", Parent: "Inception Node").
*   **Benefit:** The LLM can view the *structure* of its knowledge (the tree skeleton) without reading the entire *content*, allowing it to plan complex multi-hop paths without context saturation.

### B. Multi-Resolution Retrieval
The agent is forbidden from reading a full URL blindly. It must interact with data at different resolutions:
1.  **Macro-Resolution (Search):** Snippets and Titles.
2.  **Meso-Resolution (Inspect):** The Table of Contents (Section Headers) and Lead Paragraph.
3.  **Micro-Resolution (Read):** The raw text of a *single* specific section.

### C. JSON-Driven Autonomy
The Python code no longer controls the flow via `for` loops. The `ReasoningEngine` acts as a runtime environment that executes **JSON instructions** generated by the LLM. The LLM is fully responsible for the control flow (deciding when to search vs. when to read vs. when to stop).

---

## 2. System Components

### The Brain: `ReasoningEngine` (State Machine)
The engine runs a `while` loop that persists until the agent signals completion or safety limits are reached.
*   **Input:** Current Question + Tree View (Snapshot of memory structure) + Recent History.
*   **Process:** Invokes the LLM with temperature 0.0 to generate a structured JSON decision.
*   **Output:** Executes the requested tool and feeds the result back into the loop.

### The Memory: `ResearchTree`
A directed acyclic graph (DAG) implementation where every piece of evidence has a unique ID and a pointer to a parent.
*   **Nodes:** Contain `topic` (classification), `content` (fact), and `source_url`.
*   **Context Optimization:** The `get_tree_view()` method returns a lightweight indentation-based representation of the tree, allowing the LLM to see "what it knows" using minimal tokens.

### The Eyes: `WikipediaArticleFetcher` (Structured)
Refactored to parse HTML structure rather than just text.
*   **`get_article_structure(url)`**: Extracts `<h1>` (Title), the abstract (text before first `<h2>`), and a list of `<h2>` headers.
*   **`get_section_content(url, section_name)`**: Locates a specific header and extracts only the text between that header and the next.

---

## 3. The Cognitive Workflow

When solving a question like *"What is the capital of the birthplace of the director of Inception?"*, the system follows this non-linear path:

1.  **Initialization:** The Tree is created with a Root Node containing the Main Question.
2.  **Observation:** The Agent sees the Root Node is empty of children.
3.  **Action 1 (Search):** Query "Inception director".
4.  **Action 2 (Inspect):** Agent selects the Wikipedia URL for "Christopher Nolan" and requests the Table of Contents.
    *   *Result:* Headers ["Early Life", "Career", "Personal Life"...].
5.  **Action 3 (Read):** Agent infers that "birthplace" is likely in **"Early Life"**. It requests reading *only* that section.
6.  **Action 4 (Classify & Store):** Agent finds "Born in London". It calls `add_to_memory(parent="root", topic="Director Info", content="Nolan born in London")`.
7.  **Loop Continues:** The Agent sees the new node in the Tree. It now formulates the next hop ("London capital") based on that stored node.

---

## 4. Expected Results & Improvements

| Feature | v0.1 (Linear Pipeline) | v0.2 (Dynamic Tree Agent) |
| :--- | :--- | :--- |
| **Context Load** | **High**. Loaded full articles (10k+ tokens) indiscriminately. | **Low**. Loads only summaries and specific sections (<1k tokens). |
| **Noise** | **High**. Irrelevant sections confused the reasoning. | **Low**. Agent filters noise *before* reading. |
| **Reasoning** | Hardcoded decomposition steps. | Emergent reasoning. Agent adapts if a path fails. |
| **Precision** | Prone to "Lost-in-the-Middle" errors. | High precision due to focused reading and explicit storage. |

## 5. Technical Constraints

*   **JSON Strictness:** The system relies entirely on the LLM outputting valid JSON. Temperature 0.0 is enforced.
*   **Memory Volatility:** The `ResearchTree` is currently session-based. It allows for deep reasoning within a question but resets for the next question to prevent cross-contamination.